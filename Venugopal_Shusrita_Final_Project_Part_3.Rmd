---
title: 'INFSCI 2595 Spring 2024 Final Project Part 3'
author: "ShusritaVenugopal"
date: "2024-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load packages
The tidyverse is loaded in the code chunk below. The visualization package, ggplot2, and the data manipulation package, dplyr, are part of the “larger” tidyverse.
```{r setup, include=FALSE}
library(tidyverse)
```
The modelr package is loaded in the code chunk below. From modelr we could use functions to calculate performance metrics for your models.
```{r, solution1}
library(modelr)
```
### Reload model
Let’s now load in that model, but assign it to a different variable name. We can read in an `.rds` file with the `readr::read_rds()` function.
```{r, reload_mod01}
train_dataset <- readr::read_rds("my_train_data.rds")
train_dataset |> glimpse()
```


I will be working with glmnet package

```{r}
library(glmnet)
```

## Create the training design matrix for model 9
```{r}
library(splines)
X09_glmnet <- model.matrix(y ~ ns(R, df = 3) + ns(G, df = 3) + ns(B, df = 3) + ns(Hue, df = 3) - 1, data = train_dataset)

X09_glmnet %>% colnames()
```
## Create the training design matrix for model 10

glmnet prefers the the intercept column of ones to not be included in the design matrix. These matrices will use the same formulation but you must remove the intercept column. 
```{r}
X10_glmnet <- model.matrix(y ~ (ns(R, df = 3) + ns(G, df = 3) + ns(B, df = 3)) * Lightness - 1, data = train_dataset)

X10_glmnet %>% colnames()
```

By default glmnet uses the lasso penalty. Fit a Lasso model by calling glmnet(). The first argument to glmnet() is the design matrix and the second argument is a regular vector for the response.

### Train a Lasso model for the model 9 and model 10 formulations

Assign the results to lasso_09 and lasso_10, respectively.
The glmnet models with the default options are fit below.
```{r}
lasso_09 <- glmnet(X09_glmnet, df$y)

lasso_10 <- glmnet(X10_glmnet, df$y)

```

### Plot the coefficient path for each Lasso model by calling the plot() function on the glmnet model object. Specify the xvar argument to be 'lambda' in the plot() call.
The coefficient path for model 9 is shown below:
```{r}
plot(lasso_09, xvar = 'lambda')
```
The plot generated by the plot() function for a Lasso model (glmnet object) with xvar = 'lambda' shows the coefficient paths for each predictor variable as a function of the tuning parameter lambda. In Lasso regularization, the penalty term is controlled by the tuning parameter lambda. As lambda increases, the penalty on the coefficients increases, leading to more coefficients being shrunk towards zero. The coefficient path plot shows how the coefficients change as lambda varies.

Important inference fron the below grahs:
1. As lambda increases, the coefficients shrink towards zero. This helps in identifying the most important variables in the model. Variables with non-zero coefficients even at higher values of lambda are considered more important.
2. Variables with non-zero coefficients at low values of lambda are selected early in the regularization process and are considered important predictors.
3. Variables with zero coefficients at low lambda may become non-zero as lambda increases, indicating that they are less important or redundant.

The coefficient path for model 10 is shown below:

```{r}
plot(lasso_10, xvar = 'lambda')
```
The effect of the regularization is easier to see with model 6. On the left hand side of the figure, with a smaller penalty there are many features.

### Identify the best 'lambda' value to use

The cv.glmnet() function will by default use 10-fold cross-validation to tune 'lambda'. The first argument to cv.glmnet() is the design matrix and the second argument is the regular vector for the response.
- Tuning the Lasso regularization strength with cross-validation using the cv.glmnet() function for each model formulation. 
- Assign the model 09 result to lasso_09_cv_tune and assign the model 10 result to lasso_10_cv_tune. Also specify the alpha argument to be 1 to make sure the Lasso penalty is applied in the cv.glmnet() call.

The model 9 formulation with the lasso penalty is tuned below. The random seed is set so that the cross-validation folds are reproducible. Random seed is set to make sure the cross-validation splits will be same each time the algorithm is run. The nfolds argument is also set to 10 even though the default is 10 folds. The alpha argument is set to 1 even though that is also the default mixing fraction value.

```{r}
set.seed(812312)

lasso_09_cv_tune <- cv.glmnet(X09_glmnet, df$y, alpha = 1, nfolds = 10)
```
The model 10 lasso penalty is tuned with 10 fold cross-validation below.
```{r}
set.seed(812312)

lasso_09_cv_tune <- cv.glmnet(X09_glmnet, df$y, alpha = 1, nfolds = 10)
```
